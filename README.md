# RAPO++: Prompting Test-Time Scaling for Text-to-Video Generation
<p align="center">
  <a href="https://arxiv.org/pdf/2504.11739" target="_blank"><img src="https://img.shields.io/badge/Paper-RAPO-red"></a>
  <a href='https://whynothaha.github.io/Prompt_optimizer/RAPO.html' target="_blank"><img src='https://img.shields.io/badge/ProjectPage-RAPO-blue'></a>
  <a href="https://arxiv.org/abs/2510.20206" target="_blank"><img src="https://img.shields.io/badge/Paper-RAPO++-red"></a>
  <a href='https://whynothaha.github.io/RAPO_plus_github/' target="_blank"><img src='https://img.shields.io/badge/ProjectPage-RAPO++-blue'></a>
</p >

<p align="center">
<strong><big>
If you find our work useful, please consider giving us a starüåü</big></strong>
</p>

## üî• News
- [2025.10.26] We plan to release RAPO++ in a few days. Please stay tuned!





## üìö AutoPage
Our website is automatically generated using our [**AutoPage**](https://mqleet.github.io/AutoPage_ProjectPage/), a multi-agent system we highly recommend for effortless academic page creation.

##  Abstract

<details><summary>CLICK for the full abstract</summary>


> The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce RAPO, a novel Retrieval-
Augmented Prompt Optimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting thesuperior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set.
Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts.
</details>


## üì• Installation
1. Clone the Repository
```
git clone https://github.com/Vchitect/RAPO.git
cd RAPO
```
2. Set up Environment
```
conda create -n RAPO python=3.10
conda activate RAPO
pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

## ü§ó Checkpoint  
Download the required model weights [RAPO](https://huggingface.co/bingjie/RAPO/tree/main), relation graph and pretrained LLM (e.g. , [
Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/tree/main) )and place them in the `ckpt/` and `relation_graph/` directory.
```
ckpt/
‚îÇ‚îÄ‚îÄ all-MiniLM-L6-v2/
‚îÇ‚îÄ‚îÄ llama3_1_instruct_lora_rewrite/
‚îÇ‚îÄ‚îÄ Mistral-7B-Instruct-v0.3/
relation_graph/
‚îÇ‚îÄ‚îÄ graph_data/
```


## üñ•Ô∏è Inference  
0. We provide the codes to compose the graph data. We provide two examples of inputs to compose the graph data (`./data/graph_test1.csv` and `./data/graph_test2.csv`). You can build a relation graph from scratch based on the constructed data:
```
python construct_graph.py
```
or you can add data based on the already constructed relation graph:
```
python add_to_graph.py
```
1. Retrieve related modifiers from relation graph. You can adjust the hyperparameters in `retrieve_modifiers.py` to modify the number of retrieval modifiers.
```
sh retrieve_modifiers.sh
```
2. Word augmentation and sentence refactoring.
```
sh word_augment.sh
sh refactoring.sh
```
3. Rewrite via instruction.
```
sh rewrite_via_instruction.sh
```


## ‚úíÔ∏è Citation
If you find our work helpful for your research, please consider giving a citation üìù

```
@article{gao2025rapopp,
  title   = {RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling},
  author  = {Gao, Bingjie and Ma, Qianli and Wu, Xiaoxue and Yang, Shuai and Lan, Guanzhou and Zhao, Haonan and Chen, Jiaxuan and Liu, Qingyang and Qiao, Yu and Chen, Xinyuan and Wang, Yaohui and Niu, Li},
  journal = {arXiv preprint arXiv:2510.20206},
  year    = {2025}
}
```
```
@InProceedings{Gao_2025_CVPR,
    author    = {Gao, Bingjie and Gao, Xinyu and Wu, Xiaoxue and Zhou, Yujie and Qiao, Yu and Niu, Li and Chen, Xinyuan and Wang, Yaohui},
    title     = {The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2025},
    pages     = {3173-3183}
}
```
